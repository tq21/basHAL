
R version 4.3.0 (2023-04-21) -- "Already Tomorrow"
Copyright (C) 2023 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(devtools)
Loading required package: usethis
> load_all()
ℹ Loading sHAL
Loading required package: Matrix
Loaded glmnet 4.1-7
Loading required package: Rcpp
hal9001 v0.4.3: The Scalable Highly Adaptive Lasso
note: fit_hal defaults have changed. See ?fit_hal for details
── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.2     ✔ readr     2.1.4
✔ forcats   1.0.0     ✔ tibble    3.2.1
✔ ggplot2   3.4.2     ✔ tidyr     1.3.0
✔ lubridate 1.9.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ tidyr::expand() masks Matrix::expand()
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
✖ tidyr::pack()   masks Matrix::pack()
✖ tidyr::unpack() masks Matrix::unpack()
ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
> 
> sHAL <- R6Class("sHAL",
+   public = list(
+     X = NULL,
+     y = NULL,
+     len_candidate_basis_set = NULL,
+     len_final_basis_set = NULL,
+     max_rows = NULL,
+     max_degree = NULL,
+     batch_size = 50,
+     n_batch = 50,
+     p = 0.1,
+     alpha = NULL,
+     V_folds = 5,
+     n_cores = 1,
+     seed = NULL,
+     weight_function = "inverse loss",
+     basis_hash_table = NULL,
+     probs_keys = NULL,
+     probs = NULL,
+     family = "gaussian",
+     best_loss = Inf,
+     best_basis_set = NULL,
+     best_loss_batch = NULL,
+     best_loss_traj = NULL,
+ 
+     initialize = function(X, y, len_candidate_basis_set, len_final_basis_set,
+                           max_rows, max_degree, batch_size = 50, n_batch = 50,
+                           p = 0.1, alpha = NULL, V_folds = 5, n_cores = 1, seed = NULL,
+                           weight_function = "inverse loss", family = "gaussian",
+                           best_loss = Inf, best_basis_set = NULL, best_loss_batch = NULL, best_loss_traj = NULL) {
+       self$X <- X
+       self$y <- y
+       self$len_candidate_basis_set <- len_candidate_basis_set
+       self$len_final_basis_set <- len_final_basis_set
+       self$max_rows <- max_rows
+       self$max_degree <- max_degree
+       self$batch_size <- batch_size
+       self$n_batch <- n_batch
+       self$p <- p
+       self$alpha <- alpha
+       self$V_folds <- V_folds
+       self$n_cores <- n_cores
+       self$seed <- seed
+       self$weight_function <- weight_function
+ 
+       self$basis_hash_table <- new.env(hash = TRUE)
+       self$probs_keys <- NULL
+       self$probs <- NULL
+       self$family <- family
+ 
+       self$best_loss <- best_loss
+       self$best_basis_set <- best_basis_set
+       self$best_loss_batch <- best_loss_batch
+       self$best_loss_traj <- best_loss_traj
+     },
+ 
+     generate_basis_set = function() {
+       # sample column indices
+       col_indices <- map(seq_len(self$len_candidate_basis_set), function(i) {
+         sample(seq_len(ncol(self$X)),
+                size = sample(seq_len(self$max_degree), size = 1),
+                replace = FALSE)
+       })
+ 
+       # sample row index, get knot points
+       knot_points <- map(col_indices, function(col_idx) {
+         as.numeric(self$X[sample(seq_len(nrow(self$X)), size = 1), col_idx])
+       })
+ 
+       # make candidate basis set
+       basis_set <- map2(col_indices, knot_points, function(.x, .y) {
+         Basis$new(.x, .y)
+       })
+ 
+       return(basis_set)
+     },
+ 
+     sample_basis_set = function() {
+       # sample basis keys
+       basis_keys <- sample(self$probs_keys,
+                            size = self$len_candidate_basis_set,
+                            replace = FALSE, prob = self$probs)
+       sampled_basis_set <- map(basis_keys, function(hash_key) Basis$new(hash_key))
+     },
+ 
+     update_weight = function(non_zero_basis, weight) {
+       walk(non_zero_basis, function(basis) {
+         # get hash key
+         hash_key <- basis$hash()
+ 
+         if (is.null(self$basis_hash_table[[hash_key]])) {
+           # new basis, assign weight
+           self$basis_hash_table[[hash_key]] <- weight
+         } else {
+           # existing basis, update weight
+           self$basis_hash_table[[hash_key]] <- self$basis_hash_table[[hash_key]] + weight
+         }
+       })
+     },
+ 
+     evaluate_candidate = function(basis_set) {
+       # either fit on full data or sampled data
+       row_indices <- NULL
+       if (self$max_rows >= nrow(self$X)) {
+         row_indices <- seq_len(nrow(self$X))
+       } else {
+         row_indices <- sample(x = 1:nrow(self$X), size = self$max_rows)
+       }
+ 
+       # make design matrix
+       basis_matrix <- make_design_matrix(basis_set, as.data.frame(self$X[row_indices, ]))
+ 
+       # train-validation split
+       train_indices <- sample(x = 1:nrow(basis_matrix),
+                               size = floor(0.8 * nrow(basis_matrix)))
+       X_train <- basis_matrix[train_indices, ]
+       X_valid <- basis_matrix[-train_indices, ]
+       y_train <- self$y[row_indices][train_indices]
+       y_valid <- self$y[row_indices][-train_indices]
+ 
+       # fit CV lasso on training set, evaluate loss on validation set
+       lasso <- cv.glmnet(X_train, y_train, alpha = 1, nfold = self$V_folds, family = self$family)
+       preds <- predict(lasso, newx = X_valid, s = lasso$lambda.min, type = "response")
+       loss <- ifelse(self$family == "gaussian",
+                      sqrt(mean((y_valid - preds)^2)),
+                      -mean(y_valid * log(preds) + (1 - y_valid) * log(1 - preds)))
+ 
+       # get basis with non-zero coefficients
+       coef <- coef(lasso, s = lasso$lambda.min)[-1]
+       non_zero_basis <- basis_set[which(coef != 0)]
+ 
+       # null loss
+       null_loss <- ifelse(self$family == "gaussian",
+                           sqrt(mean((y_valid - mean(y_valid))^2)),
+                           -mean(y_valid * log(mean(y_valid)) + (1 - y_valid) * log(1 - mean(y_valid))))
+ 
+       # calculate weight using the specified weight function
+       weight <- NULL
+       if (self$weight_function == "inverse loss") {
+         weight <- inverse_loss_weight(loss)
+       } else if (self$weight_function == "double weight") {
+         weight <- double_weight(length(non_zero_basis),
+                                 self$len_candidate_basis_set,
+                                 loss,
+                                 null_loss)
+       } else if (self$weight_function == "double weight v2") {
+         weight <- double_weight_v2(length(non_zero_basis),
+                                    self$len_candidate_basis_set,
+                                    loss,
+                                    null_loss)
+       } else if (self$weight_function == "double weight v3") {
+         weight <- double_weight_v3(length(non_zero_basis),
+                                    self$len_candidate_basis_set,
+                                    loss,
+                                    null_loss)
+       }
+ 
+       return(list(non_zero_basis, weight, loss))
+     },
+ 
+     get_top_k = function(n_sample) {
+ 
+       hash_keys <- names(self$basis_hash_table)
+       loss_vals <- mget(hash_keys, envir = self$basis_hash_table,
+                         inherits = FALSE)
+       sorted_keys <- hash_keys[order(unlist(loss_vals), decreasing = TRUE)]
+       top_keys <- sorted_keys[1:ifelse(length(sorted_keys) < n_sample,
+                                        length(sorted_keys), n_sample)]
+       top_basis_set <- map(top_keys, function(hash_key) Basis$new(hash_key))
+ 
+       return(top_basis_set)
+     },
+ 
+     fit_sampled_basis_set = function(sampled_basis_set) {
+       basis_matrix <- make_design_matrix(sampled_basis_set, self$X)
+       cv_fit <- cv.glmnet(basis_matrix, self$y, nfolds = self$V_folds, alpha = 1)
+ 
+       return(cv_fit)
+     },
+ 
+     run = function(verbose = FALSE, plot = FALSE) {
+       dict_length <- vector()
+ 
+       pb <- progress_bar$new(format = "[:bar] Batch: :current of :total, time elapsed: :elapsedfull",
+                              total = self$n_batch)
+ 
+       for (i in seq_len(self$n_batch)) {
+         dict_length <- c(dict_length, length(self$basis_hash_table))
+ 
+         if (verbose) {
+           pb$tick()
+         }
+         print("iteration: " %+% i %+% ", best loss: " %+% self$best_loss)
+ 
+         # generate random candidate basis sets
+         n_random <- ifelse(is.null(self$probs),
+                            self$batch_size,
+                            round(self$batch_size * self$p))
+         random_basis_sets <- map(seq_len(n_random),
+                                  function(x) self$generate_basis_set())
+ 
+         # generate candidate basis sets from sampling distribution
+         n_sampling <- self$batch_size - n_random
+         sampled_basis_sets <- map(seq_len(n_sampling),
+                                   function(x) self$sample_basis_set())
+ 
+         # evaluate candidate basis sets in parallel
+         eval_res <- mclapply(c(random_basis_sets, sampled_basis_sets), self$evaluate_candidate, mc.cores = self$n_cores)
+ 
+         # update basis weights
+         walk(eval_res, function(.x) {
+           self$update_weight(.x[[1]], .x[[2]])
+         })
+ 
+         # keep track of best loss
+         cur_loss <- map(eval_res, function(.x) {
+           return(.x[[3]])
+         })
+         cur_best_loss <- min(unlist(cur_loss))
+         if (cur_best_loss < self$best_loss) {
+           self$best_loss <- cur_best_loss
+           self$best_basis_set <- c(random_basis_sets, sampled_basis_sets)[[which.min(unlist(cur_loss))]]
+           self$best_loss_batch <- i
+         }
+ 
+         self$best_loss_traj <- c(self$best_loss_traj, self$best_loss)
+ 
+         # update basis keys and their sampling distribution
+         self$probs_keys <- names(self$basis_hash_table)
+         weights <- unlist(mget(self$probs_keys, envir = self$basis_hash_table))
+ 
+         # normalize weight to 0-1
+         weights <- (weights - min(weights)) / (max(weights) - min(weights))
+         self$probs <- weights / sum(weights)
+       }
+ 
+       # sampled_basis_set <- self$get_top_k(self$len_final_basis_set)
+       sampled_basis_set <- self$best_basis_set
+ 
+       # fit CV Lasso on the sampled basis set
+       final_lasso <- self$fit_sampled_basis_set(sampled_basis_set)
+ 
+       # plot length of dictionary
+       if (plot) {
+         plot(dict_length, xlab = "Batch", ylab = "Dictionary length", type = 'l')
+       }
+ 
+       return(list(final_lasso, sampled_basis_set))
+     }
+   )
+ )
> 
> # load data
> dt <- read.csv("../../data/small_data/cpu.csv")
> dt <- drop_na(dt)
> y_col_idx <- 1
> x_col_idx <- setdiff(seq_len(ncol(dt)), y_col_idx)
> 
> # train-test split
> set.seed(123)
> train_indices <- sample(x = 1:nrow(dt), size = floor(0.8 * nrow(dt)))
> dt_train <- dt[train_indices, ]
> dt_test <- dt[-train_indices, ]
> 
> # run sHAL -----------------------------------------------------------------
> sHAL_obj <- sHAL$new(X = dt_train[, x_col_idx],
+                      y = dt_train[, y_col_idx],
+                      len_candidate_basis_set = 200,
+                      len_final_basis_set = 200,
+                      max_rows = 209,
+                      max_degree = 6,
+                      batch_size = 100,
+                      n_batch = 100,
+                      p = 0.5,
+                      seed = 123,
+                      weight_function = "inverse loss",
+                      family = "gaussian",
+                      n_cores = 32)
> sHAL_res <- sHAL_obj$run(verbose = TRUE, plot = FALSE)
[1] "iteration: 1, best loss: Inf"
[1] "iteration: 2, best loss: 26.8575150949944"
[1] "iteration: 3, best loss: 20.4977643846973"
[1] "iteration: 4, best loss: 20.4977643846973"
[1] "iteration: 5, best loss: 18.05352828887"
[1] "iteration: 6, best loss: 18.05352828887"
[1] "iteration: 7, best loss: 18.05352828887"
[1] "iteration: 8, best loss: 18.05352828887"
[1] "iteration: 9, best loss: 18.05352828887"
[1] "iteration: 10, best loss: 17.7160898236265"
[1] "iteration: 11, best loss: 17.7160898236265"
[1] "iteration: 12, best loss: 17.7160898236265"
[1] "iteration: 13, best loss: 17.6055202121613"
[1] "iteration: 14, best loss: 17.6055202121613"
[1] "iteration: 15, best loss: 17.6055202121613"
[1] "iteration: 16, best loss: 17.6055202121613"
[1] "iteration: 17, best loss: 17.6055202121613"
[1] "iteration: 18, best loss: 16.0479346345393"
[1] "iteration: 19, best loss: 16.0479346345393"
[1] "iteration: 20, best loss: 16.0479346345393"
[1] "iteration: 21, best loss: 16.0479346345393"
[1] "iteration: 22, best loss: 16.0479346345393"
[1] "iteration: 23, best loss: 16.0479346345393"
[1] "iteration: 24, best loss: 16.0479346345393"
[1] "iteration: 25, best loss: 16.0479346345393"
[1] "iteration: 26, best loss: 16.0479346345393"
[1] "iteration: 27, best loss: 16.0479346345393"
[1] "iteration: 28, best loss: 16.0479346345393"
[1] "iteration: 29, best loss: 16.0479346345393"
[1] "iteration: 30, best loss: 16.0479346345393"
[1] "iteration: 31, best loss: 16.0479346345393"
[1] "iteration: 32, best loss: 16.0479346345393"
[1] "iteration: 33, best loss: 16.0479346345393"
[1] "iteration: 34, best loss: 16.0479346345393"
[1] "iteration: 35, best loss: 16.0479346345393"
[1] "iteration: 36, best loss: 16.0479346345393"
[1] "iteration: 37, best loss: 16.0479346345393"
[1] "iteration: 38, best loss: 16.0479346345393"
[1] "iteration: 39, best loss: 16.0479346345393"
[1] "iteration: 40, best loss: 16.0479346345393"
[1] "iteration: 41, best loss: 16.0479346345393"
[1] "iteration: 42, best loss: 16.0479346345393"
[1] "iteration: 43, best loss: 16.0479346345393"
[1] "iteration: 44, best loss: 15.7558731305159"
[1] "iteration: 45, best loss: 15.7558731305159"
[1] "iteration: 46, best loss: 15.7558731305159"
[1] "iteration: 47, best loss: 15.7558731305159"
[1] "iteration: 48, best loss: 15.7558731305159"
[1] "iteration: 49, best loss: 15.7558731305159"
[1] "iteration: 50, best loss: 15.7558731305159"
[1] "iteration: 51, best loss: 15.7558731305159"
[1] "iteration: 52, best loss: 15.7558731305159"
[1] "iteration: 53, best loss: 15.7558731305159"
[1] "iteration: 54, best loss: 15.7558731305159"
[1] "iteration: 55, best loss: 15.7558731305159"
[1] "iteration: 56, best loss: 15.7558731305159"
[1] "iteration: 57, best loss: 15.7558731305159"
[1] "iteration: 58, best loss: 15.7558731305159"
[1] "iteration: 59, best loss: 15.7558731305159"
[1] "iteration: 60, best loss: 15.7558731305159"
[1] "iteration: 61, best loss: 15.7558731305159"
[1] "iteration: 62, best loss: 15.7558731305159"
[1] "iteration: 63, best loss: 15.7558731305159"
[1] "iteration: 64, best loss: 15.7558731305159"
[1] "iteration: 65, best loss: 15.7558731305159"
[1] "iteration: 66, best loss: 15.7558731305159"
[1] "iteration: 67, best loss: 15.7558731305159"
[1] "iteration: 68, best loss: 15.7558731305159"
[1] "iteration: 69, best loss: 15.7558731305159"
[1] "iteration: 70, best loss: 15.7558731305159"
[1] "iteration: 71, best loss: 15.7558731305159"
[1] "iteration: 72, best loss: 15.7558731305159"
[1] "iteration: 73, best loss: 15.7558731305159"
[1] "iteration: 74, best loss: 15.7558731305159"
[1] "iteration: 75, best loss: 15.7558731305159"
[1] "iteration: 76, best loss: 15.7558731305159"
[1] "iteration: 77, best loss: 15.7558731305159"
[1] "iteration: 78, best loss: 15.7558731305159"
[1] "iteration: 79, best loss: 15.7558731305159"
[1] "iteration: 80, best loss: 15.7558731305159"
[1] "iteration: 81, best loss: 15.7558731305159"
[1] "iteration: 82, best loss: 15.4868225188049"
[1] "iteration: 83, best loss: 15.4868225188049"
[1] "iteration: 84, best loss: 15.4868225188049"
[1] "iteration: 85, best loss: 15.4868225188049"
[1] "iteration: 86, best loss: 15.4868225188049"
[1] "iteration: 87, best loss: 15.4868225188049"
[1] "iteration: 88, best loss: 15.4868225188049"
[1] "iteration: 89, best loss: 15.4868225188049"
[1] "iteration: 90, best loss: 15.4868225188049"
[1] "iteration: 91, best loss: 15.4868225188049"
[1] "iteration: 92, best loss: 15.4868225188049"
[1] "iteration: 93, best loss: 15.4868225188049"
[1] "iteration: 94, best loss: 15.4868225188049"
[1] "iteration: 95, best loss: 15.4868225188049"
[1] "iteration: 96, best loss: 15.4868225188049"
[1] "iteration: 97, best loss: 15.4868225188049"
[1] "iteration: 98, best loss: 15.4868225188049"
[1] "iteration: 99, best loss: 15.4868225188049"
[1] "iteration: 100, best loss: 15.4868225188049"
> sHAL_lasso <- sHAL_res[[1]]
> sHAL_basis_set <- sHAL_res[[2]]
> 
> # get rmse
> basis_matrix_test <- make_design_matrix(sHAL_basis_set, dt_test[, x_col_idx])
> sHAL_pred <- predict(sHAL_lasso, newx = basis_matrix_test, type = "response")
> sHAL_true <- dt_test[, y_col_idx]
> print("RMSE: " %+% sqrt(mean((sHAL_pred - sHAL_true)^2)))
[1] "RMSE: 84.4328057735025"
> 
> # get number of non zero coefficients
> print("Non zero coefficients: " %+% sum(coef(sHAL_lasso) != 0))
[1] "Non zero coefficients: 68"
> 
> proc.time()
     user    system   elapsed 
11896.467  8792.068  2555.447 
