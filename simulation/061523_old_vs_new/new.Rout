
R version 4.3.0 (2023-04-21) -- "Already Tomorrow"
Copyright (C) 2023 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(devtools)
Loading required package: usethis
> load_all()
ℹ Loading sHAL
Loading required package: Matrix
Loaded glmnet 4.1-7
Loading required package: Rcpp
hal9001 v0.4.3: The Scalable Highly Adaptive Lasso
note: fit_hal defaults have changed. See ?fit_hal for details
── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.2     ✔ readr     2.1.4
✔ forcats   1.0.0     ✔ tibble    3.2.1
✔ ggplot2   3.4.2     ✔ tidyr     1.3.0
✔ lubridate 1.9.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ tidyr::expand() masks Matrix::expand()
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
✖ tidyr::pack()   masks Matrix::pack()
✖ tidyr::unpack() masks Matrix::unpack()
ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
> 
> # load data
> dt <- read.csv("../../data/small_data/cpu.csv")
> dt <- drop_na(dt)
> y_col_idx <- 1
> x_col_idx <- setdiff(seq_len(ncol(dt)), y_col_idx)
> 
> # train-test split
> set.seed(123)
> train_indices <- sample(x = 1:nrow(dt), size = floor(0.8 * nrow(dt)))
> dt_train <- dt[train_indices, ]
> dt_test <- dt[-train_indices, ]
> 
> # run sHAL -----------------------------------------------------------------
> sHAL_obj <- sHAL$new(X = dt_train[, x_col_idx],
+                      y = dt_train[, y_col_idx],
+                      len_candidate_basis_set = 200,
+                      len_final_basis_set = 200,
+                      max_rows = 209,
+                      max_degree = 6,
+                      batch_size = 100,
+                      n_batch = 100,
+                      p = 0.5,
+                      seed = 123,
+                      weight_function = "inverse loss",
+                      family = "gaussian",
+                      n_cores = 32)
> sHAL_res <- sHAL_obj$run(verbose = TRUE, plot = FALSE)
[1] "iteration: 1, best loss: Inf"
[1] "iteration: 2, best loss: 20.2649651789029"
[1] "iteration: 3, best loss: 15.7113110421545"
[1] "iteration: 4, best loss: 15.0839539099398"
[1] "iteration: 5, best loss: 14.9809479786586"
[1] "iteration: 6, best loss: 14.8699497773915"
[1] "iteration: 7, best loss: 14.7178497203884"
[1] "iteration: 8, best loss: 14.5455406583408"
[1] "iteration: 9, best loss: 14.5455406583408"
[1] "iteration: 10, best loss: 14.5117452382288"
[1] "iteration: 11, best loss: 14.5117452382288"
[1] "iteration: 12, best loss: 14.3942535777085"
[1] "iteration: 13, best loss: 14.3942535777085"
[1] "iteration: 14, best loss: 14.3942535777085"
[1] "iteration: 15, best loss: 14.3942535777085"
[1] "iteration: 16, best loss: 14.373339341783"
[1] "iteration: 17, best loss: 14.373339341783"
[1] "iteration: 18, best loss: 14.2252810233372"
[1] "iteration: 19, best loss: 14.2252810233372"
[1] "iteration: 20, best loss: 14.2252810233372"
[1] "iteration: 21, best loss: 14.2252810233372"
[1] "iteration: 22, best loss: 14.2252810233372"
[1] "iteration: 23, best loss: 14.2252810233372"
[1] "iteration: 24, best loss: 14.2252810233372"
[1] "iteration: 25, best loss: 14.2252810233372"
[1] "iteration: 26, best loss: 14.2252810233372"
[1] "iteration: 27, best loss: 14.2252810233372"
[1] "iteration: 28, best loss: 14.2252810233372"
[1] "iteration: 29, best loss: 14.2252810233372"
[1] "iteration: 30, best loss: 14.2252810233372"
[1] "iteration: 31, best loss: 14.2252810233372"
[1] "iteration: 32, best loss: 14.2252810233372"
[1] "iteration: 33, best loss: 14.2252810233372"
[1] "iteration: 34, best loss: 14.2252810233372"
[1] "iteration: 35, best loss: 14.1557919435192"
[1] "iteration: 36, best loss: 14.1557919435192"
[1] "iteration: 37, best loss: 14.1557919435192"
[1] "iteration: 38, best loss: 14.1557919435192"
[1] "iteration: 39, best loss: 14.1557919435192"
[1] "iteration: 40, best loss: 14.1557919435192"
[1] "iteration: 41, best loss: 14.1557919435192"
[1] "iteration: 42, best loss: 14.1557919435192"
[1] "iteration: 43, best loss: 14.1557919435192"
[1] "iteration: 44, best loss: 14.1557919435192"
[1] "iteration: 45, best loss: 14.1557919435192"
[1] "iteration: 46, best loss: 14.1557919435192"
[1] "iteration: 47, best loss: 14.1557919435192"
[1] "iteration: 48, best loss: 14.1557919435192"
[1] "iteration: 49, best loss: 14.1557919435192"
[1] "iteration: 50, best loss: 14.1557919435192"
[1] "iteration: 51, best loss: 14.1557919435192"
[1] "iteration: 52, best loss: 14.1557919435192"
[1] "iteration: 53, best loss: 14.1557919435192"
[1] "iteration: 54, best loss: 14.1557919435192"
[1] "iteration: 55, best loss: 14.1557919435192"
[1] "iteration: 56, best loss: 14.1557919435192"
[1] "iteration: 57, best loss: 14.1557919435192"
[1] "iteration: 58, best loss: 14.1557919435192"
[1] "iteration: 59, best loss: 14.1557919435192"
[1] "iteration: 60, best loss: 14.1557919435192"
[1] "iteration: 61, best loss: 14.1557919435192"
[1] "iteration: 62, best loss: 14.1557919435192"
[1] "iteration: 63, best loss: 14.1557919435192"
[1] "iteration: 64, best loss: 14.1557919435192"
[1] "iteration: 65, best loss: 14.1557919435192"
[1] "iteration: 66, best loss: 14.1557919435192"
[1] "iteration: 67, best loss: 14.1557919435192"
[1] "iteration: 68, best loss: 14.1557919435192"
[1] "iteration: 69, best loss: 14.1557919435192"
[1] "iteration: 70, best loss: 14.1557919435192"
[1] "iteration: 71, best loss: 14.1557919435192"
[1] "iteration: 72, best loss: 14.1325642027058"
[1] "iteration: 73, best loss: 14.1325642027058"
[1] "iteration: 74, best loss: 14.1325642027058"
[1] "iteration: 75, best loss: 14.1325642027058"
[1] "iteration: 76, best loss: 14.1325642027058"
[1] "iteration: 77, best loss: 14.1325642027058"
[1] "iteration: 78, best loss: 14.1325642027058"
[1] "iteration: 79, best loss: 14.1325642027058"
[1] "iteration: 80, best loss: 14.1325642027058"
[1] "iteration: 81, best loss: 14.1325642027058"
[1] "iteration: 82, best loss: 14.1325642027058"
[1] "iteration: 83, best loss: 14.1325642027058"
[1] "iteration: 84, best loss: 14.1325642027058"
[1] "iteration: 85, best loss: 14.1325642027058"
[1] "iteration: 86, best loss: 14.1325642027058"
[1] "iteration: 87, best loss: 14.1325642027058"
[1] "iteration: 88, best loss: 14.1325642027058"
[1] "iteration: 89, best loss: 14.1325642027058"
[1] "iteration: 90, best loss: 14.1325642027058"
[1] "iteration: 91, best loss: 14.0248576898454"
[1] "iteration: 92, best loss: 14.0248576898454"
[1] "iteration: 93, best loss: 14.0248576898454"
[1] "iteration: 94, best loss: 14.0248576898454"
[1] "iteration: 95, best loss: 14.0248576898454"
[1] "iteration: 96, best loss: 14.0248576898454"
[1] "iteration: 97, best loss: 14.0248576898454"
[1] "iteration: 98, best loss: 14.0248576898454"
[1] "iteration: 99, best loss: 14.0248576898454"
[1] "iteration: 100, best loss: 14.0248576898454"
> sHAL_lasso <- sHAL_res[[1]]
> sHAL_basis_set <- sHAL_res[[2]]
> 
> # get rmse
> basis_matrix_test <- make_design_matrix(sHAL_basis_set, dt_test[, x_col_idx])
> sHAL_pred <- predict(sHAL_lasso, newx = basis_matrix_test, type = "response")
> sHAL_true <- dt_test[, y_col_idx]
> print("RMSE: " %+% sqrt(mean((sHAL_pred - sHAL_true)^2)))
[1] "RMSE: 112.700814253832"
> 
> # get number of non zero coefficients
> print("Non zero coefficients: " %+% sum(coef(sHAL_lasso) != 0))
[1] "Non zero coefficients: 37"
> 
> proc.time()
     user    system   elapsed 
12809.822 10326.778  2839.412 
